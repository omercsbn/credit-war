# CREDIT WAR - Reinforcement Learning Training Guide

Bu rehber, CREDIT WAR ortamÄ±nda PPO (Proximal Policy Optimization) algoritmasÄ± ile ajan eÄŸitmeyi aÃ§Ä±klar.

---

## ğŸ“¦ Kurulum

### 1. Temel BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin

```bash
# Ana projeyi yÃ¼kleyin
pip install -e .

# RL kÃ¼tÃ¼phanelerini yÃ¼kleyin
pip install -r requirements_rl.txt
```

### 2. GPU DesteÄŸi (Ä°steÄŸe BaÄŸlÄ±)

Daha hÄ±zlÄ± eÄŸitim iÃ§in PyTorch GPU versiyonunu yÃ¼kleyin:

```bash
# CUDA 11.8 iÃ§in
pip install torch --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1 iÃ§in  
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

---

## ğŸ¯ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Basit EÄŸitim (RuleBased Rakibe KarÅŸÄ±)

```bash
python train_ppo.py --opponent rulebased --timesteps 1000000
```

Bu komut:
- âœ… **1 Milyon timestep** eÄŸitim Ã§alÄ±ÅŸtÄ±rÄ±r (~25,000 episode)
- âœ… **RuleBasedAgent** rakibine karÅŸÄ± Ã¶ÄŸrenir
- âœ… Model checkpoint'lerini `./models/` klasÃ¶rÃ¼ne kaydeder
- âœ… Tensorboard loglarÄ±nÄ± `./tensorboard_logs/` klasÃ¶rÃ¼ne yazar

### EÄŸitimi Ä°zleme (Tensorboard)

```bash
tensorboard --logdir tensorboard_logs/
```

TarayÄ±cÄ±nÄ±zda `http://localhost:6006` adresini aÃ§Ä±n.

**Ä°zlenecek Metrikler:**
- `rollout/win_rate` - Kazanma oranÄ± (hedef: >50%)
- `rollout/ep_rew_mean` - Ortalama reward
- `train/policy_loss` - Policy network loss
- `train/value_loss` - Value network loss

---

## ğŸ§ª FarklÄ± Rakiplere KarÅŸÄ± EÄŸitim

### Kolay Rakip (Random)
```bash
python train_ppo.py --opponent random --timesteps 500000
```
- **Avantaj**: HÄ±zlÄ± Ã¶ÄŸrenme, yÃ¼ksek win rate
- **Dezavantaj**: ZayÄ±f stratejiler Ã¶ÄŸrenir

### Orta Zorluk (Conservative)
```bash
python train_ppo.py --opponent conservative --timesteps 1000000
```
- **Avantaj**: Dengeli Ã¶ÄŸrenme
- **Dezavantaj**: Risk yÃ¶netimi Ã¼zerine odaklanÄ±r

### Zor Rakip (Aggressor)
```bash
python train_ppo.py --opponent aggressor --timesteps 2000000
```
- **Avantaj**: Adversarial davranÄ±ÅŸ Ã¶ÄŸrenir
- **Dezavantaj**: Uzun eÄŸitim sÃ¼resi, dÃ¼ÅŸÃ¼k baÅŸlangÄ±Ã§ win rate

### En Zor Rakip (RuleBased)
```bash
python train_ppo.py --opponent rulebased --timesteps 2000000
```
- **Avantaj**: En gÃ¼Ã§lÃ¼ stratejileri Ã¶ÄŸrenir
- **Dezavantaj**: Uzun yakÄ±nsama sÃ¼resi

---

## ğŸ›ï¸ Hyperparameter Tuning

### Daha HÄ±zlÄ± Ã–ÄŸrenme (YÃ¼ksek Learning Rate)
```bash
python train_ppo.py --opponent rulebased --lr 5e-4 --timesteps 1000000
```

### Daha KararlÄ± EÄŸitim (DÃ¼ÅŸÃ¼k Learning Rate)
```bash
python train_ppo.py --opponent aggressor --lr 1e-4 --timesteps 2000000
```

### Custom Seed
```bash
python train_ppo.py --opponent rulebased --seed 999 --timesteps 1000000
```

---

## ğŸ“Š Model DeÄŸerlendirme

### Tek Rakibe KarÅŸÄ± Test

```bash
python evaluate_ppo.py --model models/ppo_rulebased_final.zip --episodes 100 --opponents rulebased
```

### TÃ¼m Rakiplere KarÅŸÄ± Test

```bash
python evaluate_ppo.py --model models/ppo_rulebased_final.zip --episodes 100
```

**Beklenen Ã‡Ä±ktÄ±:**

```
======================================================================
OVERALL SUMMARY
======================================================================

Opponent        Win%       Loss%      Draw%      Avg Reward  
----------------------------------------------------------------------
random          95.0       5.0        0.0        +0.900      
greedy          30.0       0.0        70.0       +0.300      
conservative    80.0       10.0       10.0       +0.700      
rulebased       60.0       20.0       20.0       +0.400      
aggressor       55.0       25.0       20.0       +0.300      
----------------------------------------------------------------------
AVERAGE         64.0                             +0.520      
======================================================================
```

---

## ğŸ”¬ GeliÅŸmiÅŸ KullanÄ±m

### 1. Curriculum Learning (Kademeli Zorluk)

```bash
# AdÄ±m 1: Random rakibe karÅŸÄ± temel Ã¶ÄŸren
python train_ppo.py --opponent random --timesteps 500000

# AdÄ±m 2: Conservative'e karÅŸÄ± risk yÃ¶netimi Ã¶ÄŸren
python train_ppo.py --opponent conservative --timesteps 1000000

# AdÄ±m 3: RuleBased'e karÅŸÄ± ileri strateji Ã¶ÄŸren
python train_ppo.py --opponent rulebased --timesteps 2000000
```

### 2. Self-Play (Kendi Kendine Oynama)

Self-play iÃ§in kod:

```python
from credit_war.gym_wrapper import CreditWarGymEnv
from credit_war.agents.ppo_agent import PPOAgent

# Ä°lk modeli eÄŸit
# ... (train_ppo.py ile)

# EÄŸitilmiÅŸ modeli rakip olarak kullan
trained_opponent = PPOAgent(
    model_path="models/ppo_rulebased_final.zip",
    name="PPO_Opponent"
)

# Yeni model bu rakibe karÅŸÄ± Ã¶ÄŸrenir
env = CreditWarGymEnv(opponent=trained_opponent, seed=42)
# ... (SB3 PPO training)
```

### 3. Multi-Agent RL (MARL)

Her iki ajan da aynÄ± anda Ã¶ÄŸrenir (gelecekteki Ã§alÄ±ÅŸma).

---

## ğŸ“ˆ Beklenen Ã–ÄŸrenme EÄŸrisi

### Phase 1: Random Exploration (0-100k steps)
- Win rate: 20-40%
- Model rastgele aksiyonlar dener
- **Aksiyon**: SabÄ±rlÄ± olun, loss yÃ¼ksek olabilir

### Phase 2: Basic Strategy (100k-500k steps)
- Win rate: 40-60%
- GIVE_LOAN ve REJECT arasÄ±nda denge Ã¶ÄŸrenir
- **Aksiyon**: Learning rate dÃ¼ÅŸÃ¼rmek iÃ§in iyi zaman

### Phase 3: Advanced Tactics (500k-1M steps)
- Win rate: 60-75%
- UNDERCUT ve INSURE stratejilerini Ã¶ÄŸrenir
- **Aksiyon**: Checkpoint'leri kaydedin

### Phase 4: Mastery (1M+ steps)
- Win rate: 75%+
- Rakip modelleme ve uzun vadeli planlama
- **Aksiyon**: FarklÄ± rakiplere karÅŸÄ± test edin

---

## ğŸ› Troubleshooting

### Problem: Win Rate ArtmÄ±yor

**Ã‡Ã¶zÃ¼m 1**: Learning rate azalt
```bash
python train_ppo.py --lr 1e-4 --timesteps 2000000
```

**Ã‡Ã¶zÃ¼m 2**: Daha fazla timestep
```bash
python train_ppo.py --timesteps 5000000
```

**Ã‡Ã¶zÃ¼m 3**: Daha kolay rakip seÃ§
```bash
python train_ppo.py --opponent random --timesteps 500000
```

### Problem: Training Ã‡ok YavaÅŸ

**Ã‡Ã¶zÃ¼m 1**: GPU kullan (eÄŸer mevcut)
```bash
# PyTorch CUDA yÃ¼kle
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**Ã‡Ã¶zÃ¼m 2**: Batch size artÄ±r
```python
# train_ppo.py iÃ§inde
model = PPO(..., batch_size=128, n_steps=4096)
```

### Problem: Policy Collapse (Hep AynÄ± Aksiyon)

**Ã‡Ã¶zÃ¼m**: Entropy coefficient artÄ±r
```python
# train_ppo.py iÃ§inde
model = PPO(..., ent_coef=0.05)  # 0.01'den 0.05'e
```

---

## ğŸ“š Algoritma DetaylarÄ±

### PPO Hyperparameters

| Parameter | Default | AÃ§Ä±klama |
|-----------|---------|----------|
| `learning_rate` | 3e-4 | Optimizer learning rate |
| `n_steps` | 2048 | Rollout buffer size |
| `batch_size` | 64 | Minibatch size |
| `n_epochs` | 10 | Optimization epochs per rollout |
| `gamma` | 0.99 | Discount factor |
| `gae_lambda` | 0.95 | GAE lambda |
| `clip_range` | 0.2 | PPO clip epsilon |
| `ent_coef` | 0.01 | Entropy coefficient |
| `vf_coef` | 0.5 | Value function coefficient |

### Network Architecture

**Policy Network (Actor):**
```
Input (12) â†’ Dense(256) â†’ ReLU â†’ Dense(256) â†’ ReLU â†’ Output(5) â†’ Softmax
```

**Value Network (Critic):**
```
Input (12) â†’ Dense(256) â†’ ReLU â†’ Dense(256) â†’ ReLU â†’ Output(1)
```

**Total Parameters:** ~135,000

---

## ğŸ“ Akademik KullanÄ±m

### Tez Ä°Ã§in Ã–nerilen Deney Seti

```bash
# Deney 1: Baseline (RuleBased rakip)
python train_ppo.py --opponent rulebased --timesteps 2000000 --seed 42

# Deney 2: Adversarial (Aggressor rakip)
python train_ppo.py --opponent aggressor --timesteps 2000000 --seed 42

# Deney 3: Multi-seed (reproducibility)
for seed in 42 123 999; do
    python train_ppo.py --opponent rulebased --timesteps 1000000 --seed $seed
done

# Deney 4: Curriculum Learning
python train_ppo.py --opponent random --timesteps 500000 --seed 42
# (sonra modeli fine-tune et)
```

### Metrikler ve Raporlama

Training sÄ±rasÄ±nda kayÄ±t edilen metrikler:
- `rollout/win_rate` - Kazanma oranÄ±
- `rollout/loss_rate` - Kaybetme oranÄ±
- `rollout/draw_rate` - Beraberlik oranÄ±
- `rollout/ep_len_mean` - Ortalama episode uzunluÄŸu
- `train/learning_rate` - GÃ¼ncel learning rate
- `train/entropy_loss` - Policy entropy

Tensorboard ile dÄ±ÅŸa aktarma:
```bash
tensorboard --logdir tensorboard_logs/ --logdir_spec=PPO:tensorboard_logs/PPO_rulebased_1
```

---

## ğŸš€ Ä°leri AdÄ±mlar

### 1. Multi-Agent PPO
Her iki ajan da aynÄ± anda Ã¶ÄŸrenir (karmaÅŸÄ±k!)

### 2. DQN AlgoritmasÄ±
Discrete action space iÃ§in alternatif

### 3. Partial Observability
Rakibin state'ini gizle (daha zor!)

### 4. Transfer Learning
Bir rakibe karÅŸÄ± Ã¶ÄŸrendiÄŸini baÅŸka rakibe transfer et

---

## ğŸ“– Referanslar

- **PPO Algorithm**: [Schulman et al., 2017](https://arxiv.org/abs/1707.06347)
- **Stable-Baselines3**: [Documentation](https://stable-baselines3.readthedocs.io/)
- **Gymnasium**: [Documentation](https://gymnasium.farama.org/)

---

**BaÅŸarÄ±lar! ğŸ‰**

SorularÄ±nÄ±z iÃ§in: CREDIT WAR GitHub Issues
