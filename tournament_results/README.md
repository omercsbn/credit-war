# Game Theory Agents Tournament Results

## ğŸ® Oyun Teorisi AjanlarÄ± - Battle Royale SonuÃ§larÄ±

Bu dizin, 10 farklÄ± oyun teorisi tabanlÄ± PPO ajanÄ±nÄ±n birbirine karÅŸÄ± oynadÄ±ÄŸÄ± turnuva sonuÃ§larÄ±nÄ± iÃ§erir.

### ğŸ“Š Dosyalar

#### GÃ¶rselleÅŸtirmeler
- **`win_rate_heatmap.png`** - Kazanma oranlarÄ± heatmap'i (10x10 matrix)
  - SatÄ±r: Agent A (oyuncu)
  - SÃ¼tun: Agent B (rakip)
  - Renk: YeÅŸil = YÃ¼ksek kazanma, KÄ±rmÄ±zÄ± = DÃ¼ÅŸÃ¼k kazanma
  - En deÄŸerli gÃ¶rselleÅŸtirme: Hangi strateji hangi stratejiye karÅŸÄ± etkili?

- **`reward_heatmap.png`** - Ortalama Ã¶dÃ¼l heatmap'i
  - Average reward per episode (-1 ile +1 arasÄ±)
  - Daha detaylÄ± performans Ã¶lÃ§Ã¼mÃ¼

#### Veri DosyalarÄ± (CSV)
- **`win_rate_matrix.csv`** - Kazanma oranlarÄ± matrisi (Excel'de aÃ§Ä±labilir)
- **`reward_matrix.csv`** - Ã–dÃ¼l matrisi
- **`elo_ratings.csv`** - ELO sÄ±ralamasÄ±
  - SatranÃ§ tarzÄ± rating sistemi
  - 1500 = ortalama
  - >1600 = elite
  - <1400 = zayÄ±f

#### Ham Veri
- **`tournament_results.json`** - TÃ¼m matchup'larÄ±n detaylÄ± sonuÃ§larÄ±
  - Her matchup iÃ§in: wins, losses, draws, win_rate, avg_reward

### ğŸ† Turnuva FormatÄ±

**Round-Robin (Herkes Herkese KarÅŸÄ±)**
- 10 ajan Ã— 9 rakip = 90 matchup
- Her matchup: 50 episode
- Toplam: 4,500 episode

**Ajan Listesi:**
1. **nash** - Nash Equilibrium Banker (dengeli mix stratejiler)
2. **titfortat** - Tit-for-Tat Banker (karÅŸÄ±lÄ±klÄ±lÄ±k, affedici)
3. **grimtrigger** - Grim Trigger Bank (ihanet = sonsuz ceza)
4. **bayesian** - Adaptive Bayesian Bank (inanÃ§ gÃ¼ncelleme)
5. **predator** - Predator Bank (zayÄ±f avcÄ±sÄ±)
6. **minimax** - Risk-Averse Regulator Bank (hayatta kalma)
7. **switcher** - Opportunistic Switcher (rejim deÄŸiÅŸtirme)
8. **evolutionary** - Evolutionary Learner (baÅŸarÄ±yÄ± taklit)
9. **zerosum** - Zero-Sum Warrior (rÃ¶latif Ã¼stÃ¼nlÃ¼k)
10. **metalearner** - Meta-Learner Bank (rakip sÄ±nÄ±flandÄ±rma)

### ğŸ“ˆ SonuÃ§larÄ± Yorumlama

#### Heatmap NasÄ±l Okunur?
```
         bayesian  predator  nash  ...
bayesian    -        85%     60%   ...   <- bayesian satÄ±rÄ±
predator   15%       -       40%   ...
nash       40%      60%       -    ...
```

- **SatÄ±r (Agent A)**: Oyuncunun kazanma oranÄ±
- **SÃ¼tun (Agent B)**: Rakip
- **Ã–rnek**: bayesian vs predator â†’ 85% kazanÄ±yor

#### Dikkat Edilmesi Gerekenler

1. **Asimetri**: A vs B â‰  B vs A (simultaneous action environment)
2. **Rock-Paper-Scissors**: DÃ¶ngÃ¼sel Ã¼stÃ¼nlÃ¼kler var mÄ±?
   - Ã–rnek: A > B, B > C, C > A
3. **Dominant Strategy**: TÃ¼m rakiplere karÅŸÄ± kazanan var mÄ±?
4. **Exploitable Weakness**: Herkes tarafÄ±ndan yenilen var mÄ±?

### ğŸ”¬ Akademik KatkÄ±lar

**Bu turnuva ÅŸunlarÄ± gÃ¶sterir:**
1. **Nash Equilibrium Testi**: Hangi strategi diÄŸerlerine karÅŸÄ± stabil?
2. **Evolutionary Stability**: Uzun vadede hangi strateji hayatta kalÄ±r?
3. **Opponent Modeling**: Meta-learner gerÃ§ekten adapte oluyor mu?
4. **Risk-Return Tradeoff**: Minimax vs Predator karÅŸÄ±laÅŸtÄ±rmasÄ±
5. **Cooperation Dynamics**: Tit-for-Tat ve Grim Trigger'Ä±n baÅŸarÄ±sÄ±

### ğŸ’¡ Tez Ä°Ã§in KullanÄ±m

**BÃ¶lÃ¼mler:**
- **3. Methodology**: "10 game theory-based agents trained via PPO with custom reward shaping"
- **4. Experiments**: "Round-robin tournament with 4,500 total episodes"
- **5. Results**: Win rate heatmap + ELO rankings
- **6. Discussion**: Strategy analysis, Nash equilibrium, dominant strategies

**Grafikler:**
- Figure 1: Win Rate Heatmap (tezde en Ã§ok yer alan gÃ¶rsel olacak)
- Figure 2: ELO Rankings Bar Chart
- Table 1: Overall Performance Statistics

### ğŸš€ Ä°leri Analiz

**YapÄ±labilecekler:**
```python
import pandas as pd
import numpy as np

# Load results
df = pd.read_csv('win_rate_matrix.csv', index_col=0)

# Find most balanced agent (closest to 50% against all)
balance_scores = np.abs(df.mean(axis=1) - 50)
most_balanced = balance_scores.idxmin()

# Find rock-paper-scissors cycles
# A beats B, B beats C, C beats A

# Calculate strategy diversity
# How different are the learned policies?
```

### ğŸ“š Referanslar

**Game Theory Concepts:**
- Nash Equilibrium (Nash, 1950)
- Evolutionary Game Theory (Maynard Smith, 1982)
- Tit-for-Tat (Axelrod, 1984)
- Bayesian Games (Harsanyi, 1967)
- Minimax Theorem (von Neumann, 1928)

**MARL References:**
- Multi-Agent Reinforcement Learning (Busoniu et al., 2008)
- Opponent Modeling (Albrecht & Stone, 2018)
- Nash-PPO (OpenAI, 2021)

---

**Generated by:** `tournament_battle_royale.py`
**Date:** January 2026
**Framework:** Stable-Baselines3 PPO + Custom Reward Shaping
